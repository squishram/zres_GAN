{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "from matplotlib.pyplot import *\n",
    "from matplotlib import animation\n",
    "import ipywidgets as widgets\n",
    "import math, random, torch, numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network\n",
    "\n",
    "A neural network is a flexible parameterised function used for fitting data. Typically it consists of a number of layers. Usually, each layer performs a linear transformation on its input followed by a nonlinear function. The input to a layer is an $N$ dimensional vector $\\mathbf{x}$, and the output is a $M$ dimensional vetor $\\mathbf{y}$. The transformation is usually of the form:\n",
    "$$\n",
    "\\mathbf{y} = S(\\mathbf{W}x + \\mathbf{b})\n",
    "$$\n",
    "where $\\mathbf{W}$ is an $N\\times M$ matrix of weights, $\\mathbf{b}$ is an $M$ dimensional vector of biases and $S$ applies a nonlinear function independently to each element of its input. Neural network consists of several (or many) layers. The function $S$ might vary between layers, and often $M$ has various special forms. This structure, consisted of a stack of layers is known as a *feedforward neural network*.\n",
    "\n",
    "This is a bit abstract so we'll work through one from scratch.\n",
    "\n",
    "First let's start by making some noisy simulated data and fitting it with a neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6fe0b1ffee0e4005abce74838d9ef9dd"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "x = torch.tensor(range(0,4000))/4000\n",
    "y = torch.tensor([math.sin(i*32*2/math.pi) + i*8 + random.random() for i in x])\n",
    "figure()\n",
    "plot(x, y)\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the linear transform, or $\\mathbf{W}x + \\mathbf{b}$ bit above.\n",
    "\n",
    "First an important thing to note. For this to be practical we need to write a function that operates on multiple data elements in one go. This is known as a batch of data. In pytorch, by convention the first dimension of a tensor is the batch if applicable.\n",
    "\n",
    "Beyond that, we can do the computation with a combination of adding dimensions, duplicating the data (using `expand` this is free in pytorch and crucially keeps derivatives, generally you need to be careful with gradients when copying data), elementwise multiplication and addition of tensors and summing along dimension. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_transform(x, weights, bias):\n",
    "    # B is the batch size, C is the number of channels in, D is the number of channels out\n",
    "    # x.size() == [B,C]\n",
    "    # weights.size() == [C,D]\n",
    "    # bias.size() == D\n",
    "    assert x.ndim == 2\n",
    "    assert weights.ndim == 2\n",
    "    assert bias.ndim == 1\n",
    "    [B, C] = x.size()\n",
    "    [D] = bias.size()\n",
    "    assert list(weights.size()) == [C,D]\n",
    "    \n",
    "    # x1.size() == [B,C,1]\n",
    "    x1 = x.unsqueeze(2)\n",
    "    \n",
    "    # x2.size() == [B, C, D], where x is effectively duplicated D times\n",
    "    x2 = x1.expand([B, C, D])\n",
    "    \n",
    "    #Ignoring the batch dimension, if x = [1, 2] and D is 3 then x2 is\n",
    "    #\n",
    "    # [[1 1 1]\n",
    "    #  [2 2 2]]\n",
    "    #\n",
    "    # Note that I've drawn the last dimension as a row. So a 1D tensor is \n",
    "    # just a row. A 2D tensor has thw row indexed first then the column within the row\n",
    "    \n",
    "    # Duplicate weights and biases across the batch dimension\n",
    "    weights2 = weights.unsqueeze(0).expand([B, C, D])\n",
    "    # Ignoring the batch, the weights, w, may be:\n",
    "    # [[1 2 3]\n",
    "    #  [4 5 6]]\n",
    "    \n",
    "    bias2 = bias.unsqueeze(0).expand([B, D])\n",
    "    \n",
    "    # x2 * weights will then be:\n",
    "    # [1  2  3]\n",
    "    # [8 10 12]\n",
    "    # We then sum down dimension 1, which since we're ignoring the batch gives:\n",
    "    # [9 12 15]\n",
    "    linear = (x2 * weights).sum(1) + bias2\n",
    "    return linear\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I've made a basic linear transform layer, the next step is to use it to create a network. For this we'll need the parameters in this case, the weights and biases of each layer, and of course a function that applies the layers and a nonlinear transform. I've used two different nonlinear transforms, ReLU and Sigmoid:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "47ff17a13b904fbb96e26fd8af0502ba"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "xval = torch.tensor(np.arange(-10,10,0.01))\n",
    "figure()\n",
    "#sigmoids map all values from -inf < z < +inf to 0 < a < 1\n",
    "plot(xval, torch.sigmoid(xval), 'r', label=\"Sigmoid\")\n",
    "#relus map all <0 values to 0 and leave the rest unchanged\n",
    "plot(xval, torch.relu(xval), 'b', label=\"relu\")\n",
    "axis([-10, 10, 0, 4])\n",
    "legend()\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many other types too. \n",
    "\n",
    "The most convient way to build the network is to use a class to store the parameters. I've defined a `forward` function to apply the net to an input and `parameters` to return all the parameters in a list, so I can pass them to an optimizer. I've defined a 4 layer network (the inputs are conventionally called a layer too). The last layer doesn't have a nonlinearity applied, since I don't want to limit the range of the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class network:\n",
    "    def __init__(self):\n",
    "        #Input one channel, output 40 channels\n",
    "        self.layer1_weights = torch.rand([1, 40])-.5\n",
    "        self.layer1_bias = torch.rand([40])-.5\n",
    "        \n",
    "        #Input 40 channels, output 60\n",
    "        self.layer2_weights = torch.rand([40, 60])-.5\n",
    "        self.layer2_bias = torch.rand([60])-.5\n",
    "        \n",
    "        #input 60 channels, output 1\n",
    "        self.layer3_weights = torch.rand([60, 1])-.5\n",
    "        self.layer3_bias = torch.rand([1])-.5\n",
    "        \n",
    "        \n",
    "        for i in self.parameters():\n",
    "            i.requires_grad = True\n",
    "    def parameters(self):\n",
    "        return [\n",
    "            self.layer1_weights, self.layer1_bias,\n",
    "            self.layer2_weights, self.layer2_bias,\n",
    "            self.layer3_weights, self.layer3_bias,\n",
    "        ]\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Each datapoint x is scalar, so X is a 1D tensor containing a batch of B datapoints\n",
    "        # linear_transform expects a 2D input, so this transforms it into a 2D, Bx1 tensor\n",
    "        l0 = x.unsqueeze(1) \n",
    "        l1 = torch.relu(linear_transform(l0, self.layer1_weights, self.layer1_bias))\n",
    "        l2 = torch.sigmoid(linear_transform(l1, self.layer2_weights, self.layer2_bias))\n",
    "        l3 = (linear_transform(l2, self.layer3_weights, self.layer3_bias))\n",
    "        # We want the output to be scalar for each datapoint, so we drop the last dimension\n",
    "        # of the 2D Bx1 tensor giving a 1D B dimensional tensor\n",
    "        return l3.squeeze(1)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to use the network.\n",
    "\n",
    "I've implemented optimization using Stochastic Gradient Descent variant Adam. For SGD, optimization also only proceeds on a small subset of the data at each iteration. This has two advantages: one is that you don't need all the data at once which helps if the dataset is large, the other is that it is much more resistant to getting stuck in local minima. I've cheated and used a random subset with replacement each time. In reality you would pihc random subsets without replacement until all the data is exhausted (that's called an Epoch) and then repeat for many epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "126474d6a87449f5b871c2f59fffcc4a"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Button(description='Click me a lot', style=ButtonStyle())",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4d599f987f434315b0205ed77b1a3568"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Output()",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fb613f9e9fe34141958f280901c22308"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "net = network()\n",
    "\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.1)\n",
    "\n",
    "figure()\n",
    "def update_graph(_):\n",
    "    global net, x, y, optimizer\n",
    "    clf()\n",
    "    \n",
    " \n",
    "    #Gradient descent is slow, so run for many iterations on each button press \n",
    "    for i in range(100):\n",
    "        # pick a random subset of the data\n",
    "        ind = random.sample(list(range(len(x))), 100)\n",
    "        xr = x[ind]\n",
    "        yr = y[ind]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        y_pred = net.forward(xr)\n",
    "        # Least squares fitting\n",
    "        loss = ((yr - y_pred)**2).sum() / len(yr)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    title(str(loss))\n",
    "    plot(x, y)\n",
    "    plot(x, net.forward(x).detach().numpy(), 'r')\n",
    "    \n",
    "\n",
    "    \n",
    "update_graph(None)\n",
    "button = widgets.Button(description=\"Click me a lot\")\n",
    "output = widgets.Output()\n",
    "display(button, output)\n",
    "button.on_click(update_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And now in pytorch\n",
    "\n",
    "Much like gradients and optimizers, PyTorch has a lot of tools and features to make this process easier and take out the repetitive code. The first part is dealing wiht the data. I avoided writing a proper SGD because it's a bit awkward.  In PyTorch, I'll do it properly.\n",
    "\n",
    "### Data\n",
    "\n",
    "There are two important concepts:\n",
    "* Datasets\n",
    "* Data Loaders\n",
    "\n",
    "A *dataset* is simply something you can index to get an item of data. A list could be a dataset. In a case like ours where we're fitting a function, a datum has two elements, the $x$ and $y$ coordinates. Since they are going to be plumbed into pytorch, you want them to come in as tensors. We could turn `x` and `y` into a dataset like this, where each datum is a tuple containg x and y each as a tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(tensor(0.), tensor(0.8140))\n"
     ]
    }
   ],
   "source": [
    "dataset = [ (xe, ye) for (xe, ye) in zip(x, y)]\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It probably won't come as much surprise that pytorch can already assemble data held in tensors into a dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(tensor(0.), tensor(0.8140))\n"
     ]
    }
   ],
   "source": [
    "dataset = torch.utils.data.TensorDataset(x, y)\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a good chance you will need to write a data set.\n",
    "\n",
    "The next concept is a *data loader*. This is responsible for loading data in a useful way: it fetches data from the provided dataset, assembles it into batches, and can optional randomize the order for a proper implementation of SGD (as well as many other things). You can even perform transformations on your data, e.g. converting it to tensors if the dataset is in the wrong format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[tensor([0.0027, 0.2025, 0.6432, 0.5550, 0.8813, 0.5742, 0.5347, 0.1065, 0.7265,\n        0.1200]), tensor([0.2987, 1.0748, 5.7999, 4.1302, 6.9558, 4.3736, 3.4775, 1.7983, 7.4662,\n        1.7759])]\n"
     ]
    }
   ],
   "source": [
    "loader = torch.utils.data.DataLoader(dataset, batch_size=10, shuffle=True, drop_last=True)\n",
    "print(loader.__iter__().__next__()) #This incantation prints the first item that would be fetched by a for loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see it's loaded 10 random $x$ values (which is samples without replacement) and the corresponding $y$ values. Since the individual data elements were tuples of tensors, the results is tuples of tensors with a batch dimension added. `drop_last` tells the loader not to give you a partial batch at the end if the amount of data is not divisible by the batch size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layers and Networks\n",
    "\n",
    "The useful concept is a helper class called `Module`. This essentially defines `parameters()` for you and will automatically search through member variables for you and assemble all the parameters in them into a list to pass to the optimizer. Since it can get all the paramerers easily, it also gives you helper functions for loading and saving trained networks.\n",
    "\n",
    "Pytorch defines many ready made modules for you, e.g. `Linear` which implements the linear transform you have already seen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn\n",
    "class network2(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(network2, self).__init__() #Python has the best OO syntax...\n",
    "        \n",
    "        self.linear1 = torch.nn.Linear(1, 40)\n",
    "        self.linear2 = torch.nn.Linear(40, 60)\n",
    "        self.linear3 = torch.nn.Linear(60, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        l1 = torch.relu(self.linear1(x.unsqueeze(1)))\n",
    "        l2 = torch.sigmoid(self.linear2(l1))\n",
    "        l3 = self.linear3(l2).squeeze(1)\n",
    "        return l3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tqdm'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-5becfeeedad6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[1;31m#For a nice progress bar\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mloader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdrop_last\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mnet2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnetwork2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m.1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tqdm'"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm #For a nice progress bar\n",
    "loader = torch.utils.data.DataLoader(dataset, batch_size=100, shuffle=True, drop_last=True)\n",
    "net2 = network2()\n",
    "optimizer = torch.optim.Adam(net2.parameters(), lr=.1)\n",
    "for epoch in tqdm(range(100)):\n",
    "    for xdata, ydata in loader:\n",
    "        optimizer.zero_grad();\n",
    "        loss = torch.nn.functional.mse_loss(net2.forward(xdata), ydata)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "figure()\n",
    "plot(x, y, 'b')\n",
    "plot(x, net2.forward(x).detach(), 'r')\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results may be different from before due to differences in the random initialisation and sampling. Try rerunning a few times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit (windows store)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "interpreter": {
   "hash": "1f8d80d535cfd832283e4e3a1095d2ce45fe6627336684f2622a1965babb2f1c"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}