{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# So, you want to learn deep learning the hard way?\n",
    "\n",
    "\n",
    "## Getting started\n",
    "\n",
    "First, you'll need to install torch, torchvision, ipympl, ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "from matplotlib.pyplot import *\n",
    "from matplotlib import animation as ani\n",
    "import ipywidgets as widgets\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep learning in a nutshell\n",
    "\n",
    "Optimization is where you have a function and you find some parameters that find an optimum (e.g. a minimum) of the function. Machine learning is where you get to choose the function and it contains a lot of data. Deep learning is where you pick a function that isn't special in any mathematical way, use gradient descent and hope for the best.\n",
    "\n",
    "It turns out that this works quite well.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization and gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A popular function for illustrating optimization algorithms is the [Rosenbrock Banana Function](https://en.wikipedia.org/wiki/Rosenbrock_function). The equation is:\n",
    "$$\n",
    "    f = (a-x)^2 + b(y-x^2)^2\n",
    "$$\n",
    "which in code is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rosenbrock(x, y=1, a=1, b=100):\n",
    "    return (a-x)**2 + (y - x**2)**2 * b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "136ad88cf4ee4da7aa33fcf97efc684a"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "xs = np.arange(-2, 2, 0.1)\n",
    "ys = [rosenbrock(x) for x in xs]\n",
    "plot(xs, ys)\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest way to minimize this function is *gradient descent*. This is a very simple algorithm, where you simply walk down hill by going in the opposite direction of the gradient. you need a scale $\\alpha$ because the magnitute of the gradient may be large or small, so the steps may not be of a convenient size.  In maths this is:\n",
    "$$\n",
    "    x_{n+1} = x_n - \\alpha\\left.\\frac{\\text{d}f(x)}{\\text{d}x}\\right|_{x=x_n}.\n",
    "$$\n",
    "In deep learning, $\\alpha$ is known as the *learning rate*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conceptually that is simple, but annoying because you need the derivative. In the case of Rosenbrock, the equation is not too bad:\n",
    "$$\n",
    "    \\frac{\\text{d}f}{\\text{d}x} = -2(a-x) - 4bx(y-x^2)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rosenbrock_deriv_x(x, y=1, a=1, b=100):\n",
    "    return -2*(a-x) - 4*b*x*(y-x**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3f277c67f3824e82872506c0fad43d30"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Button(description='Click me to update', style=ButtonStyle())",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "24073866a4e44a7f8e7baeb0462892a5"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Output()",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "eae1130cdb9d46538adff5a05ed6fbc4"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "alpha = 0.0001\n",
    "#Start at x=-2\n",
    "x_n = -2\n",
    "fig = figure()\n",
    "axes = fig.add_subplot(111)\n",
    "\n",
    "def draw_graph(_=None):\n",
    "    global x_n, alpha\n",
    "    axes.clear()\n",
    "    axes.plot(xs, ys)\n",
    "    axes.plot(x_n, rosenbrock(x_n), 'r*')\n",
    "    \n",
    "    #Compute the next position using gradient descent\n",
    "    next_x = x_n - alpha * rosenbrock_deriv_x(x_n)\n",
    "    \n",
    "    #Plot some stuff\n",
    "    y = rosenbrock(x_n)\n",
    "    y_update = y - (x_n-next_x) * rosenbrock_deriv_x(x_n)\n",
    "    axes.plot([x_n, next_x],  [y, y_update], 'g')\n",
    "    axes.plot([next_x, next_x], [y_update, rosenbrock(next_x)], 'k:')\n",
    "    legend(['Function', 'Current x', 'Gradient'])\n",
    "    title(f\"y = {y}\")\n",
    "    \n",
    "    #Update the current x position to the new one\n",
    "    x_n = next_x\n",
    "    show()\n",
    "\n",
    "draw_graph()\n",
    "button = widgets.Button(description=\"Click me to update\")\n",
    "output = widgets.Output()\n",
    "display(button, output)\n",
    "button.on_click(draw_graph)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic differentiation\n",
    "\n",
    "Computing gradients by hand is tedious and very error prone. Fortunately there is a technique to do it automatically called *automatic differentiation*. This technique is not an automatic way of computing derivatives symbolically, it's a numeric technique based on repeated application of the chain rule (and a few others such as the product rule). This techinque forms the core of how pytorch works.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider some simple operations we might want to do. A simple one is:\n",
    "$$\n",
    "y = g(x)^2\n",
    "$$\n",
    "and it's derivative is:\n",
    "$$\n",
    "    \\frac{\\text{d}y}{\\text{d}x} = 2*g(x) \\frac{\\text{d}g}{\\text{d}x}\n",
    "$$\n",
    "The important observation is that if we already have $g(x)$ and $\\frac{\\text{d}g}{\\text{d}x}$, then we can easily compute $\\frac{\\text{d}y}{\\text{d}x}$. This is how autodiff works. Instead of just using numbers, we can use tuples that hold a number and it's derivative. So we can implement a function to square one of these,which is just an implementation of the above maths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def square(n):\n",
    "    return (n[0]*n[0], 2*n[0]*n[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do the same for other mathematical operations too. For example:\n",
    "$$\n",
    "    y = f(x) + g(x)\n",
    "$$\n",
    "and\n",
    "$$\n",
    "      \\frac{\\text{d}y}{\\text{d}x} = \\frac{\\text{d}f}{\\text{d}x} + \\frac{\\text{d}g}{\\text{d}x}\n",
    "$$\n",
    "which in code looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add(left, right):\n",
    "    return (left[0] + right[0], left[1] + right[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far so good, but how do we get started? Well, we start using a variable and it's derivative. The derivative of something with respect to itself is just 1, which is nice and simple. So if we wanted to start with $x=5$, we would just write:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = (5, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use the functions we wrote. We only have squaring and adding so far, but that's enough for a function like this:\n",
    "$$\n",
    "    y = (x^2 + x)^2 + x\n",
    "$$\n",
    "so if we use our new functions, it computes not just $y$ but its derivative automatically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(905, 661)\n"
     ]
    }
   ],
   "source": [
    "y = add(square(add(square(x), x)), x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's verify this. The symboic derivative is:\n",
    "$$\n",
    "    y =  2 (x^2 + x) (2x+1) + 1\n",
    "$$\n",
    "and running the numbers gives:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "905\n",
      "661\n"
     ]
    }
   ],
   "source": [
    "x=5\n",
    "print((x**2+x)**2 + x)\n",
    "print(2*(x**2 + x)*(2*x+1)+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Woo hoo!\n",
    "\n",
    "Using tuples and functions is hard to read because it doesn't use infix notation. We can fix this by using a class instead of a tuple and defining operators for the class. Here is an implementation of just enough of the operators to execute the Rosenbrock function. Feel free to extend it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "class AutoDiff:\n",
    "    def __init__(self, value, gradient = 1):\n",
    "        self.value = value\n",
    "        self.gradient = gradient\n",
    "        \n",
    "    def __add__(self, rhs):\n",
    "        if isinstance(rhs, AutoDiff):\n",
    "            return AutoDiff(self.value + rhs.value, self.gradient + rhs.gradient)\n",
    "        else:\n",
    "            return self + AutoDiff(rhs,0)\n",
    "        \n",
    "    def __rsub__(self, lhs):\n",
    "        return self + lhs*-1\n",
    "    \n",
    "    def __mul__(self, rhs):\n",
    "        if isinstance(rhs, AutoDiff):\n",
    "            return AutoDiff(self.value * rhs.value, self.gradient * rhs.value + self.value * rhs.gradient)\n",
    "        else:\n",
    "            return self * AutoDiff(rhs, 0)\n",
    "        \n",
    "    def __pow__(self, exponent):\n",
    "        a = self.value\n",
    "        \n",
    "        if isinstance(exponent, AutoDiff):\n",
    "            b = exponent.value\n",
    "            return AutoDiff(a**b, b*a**(b-1)*self.gradient + a**b * math.log(a) * exponent.gradient)\n",
    "        else:\n",
    "            b = exponent\n",
    "            return self ** AutoDiff(exponent,0)\n",
    "        \n",
    "    def __str__(self):\n",
    "        return str((self.value, self.gradient))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then pass this to `rosenbrock` instead of a number and it will compute the value and it's derivative:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(901, 2402.0)\n"
     ]
    }
   ],
   "source": [
    "x = AutoDiff(2)\n",
    "print(rosenbrock(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And double checking to be sure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "901 2402\n"
     ]
    }
   ],
   "source": [
    "print(rosenbrock(2), rosenbrock_deriv_x(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It works again! Mostly you won't need to know about this. However sometimes details will leak into your program, often in the forms of bugs. If you acciently convert your variable from an `AutoDiff` type to a number and use it, you can lose derivatives for some of your code without noticing, for example. Sometimes you need to do this intentionally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The same again but using pytorch\n",
    "\n",
    "Torch essentially does all of this but it is much more powerful and sophisticated and complete, so it often requires a couple more steps. In addition Torch works entirely with what it calls tensors (multidimensional arrays), so everything must be in terms of those even if you are using scalars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "901.0\n2402.0\n"
     ]
    }
   ],
   "source": [
    "import torch as t\n",
    "\n",
    "#Declare a variable and enable automatic differentation\n",
    "x = t.tensor([2.0], requires_grad=True)\n",
    "y = rosenbrock(x)\n",
    "#Perform computation of the derivative\n",
    "y.backward()\n",
    "\n",
    "print(y.item())\n",
    "#Because we called y.backward(), this is the derivative of y with respect to x\n",
    "print(x.grad.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch uses a slightly different algorithm to do automatic differentiation. You need to flag which variables you want gradients for (by using `requires_grad=True`) and then the result is computed when `backwards()` is called and the results are stored in the variable. This makes it much easier to figure out which derivative belongs to which dependent variable when there's more than one.\n",
    "\n",
    "Since pytorch can compute gradients, we can use it for gradient descent. Here is the same gradient descent algorithm implemented in pytorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9d653ef780144abfb1a19cab9410c5d6"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "-1.0686041116714478\n"
     ]
    }
   ],
   "source": [
    "figure()\n",
    "plot(xs, ys)\n",
    "\n",
    "x_n = t.tensor([-2.0])\n",
    "\n",
    "for i in range(20):\n",
    "    #We want to compute gradients with respect to x_n \n",
    "    x_n.requires_grad=True\n",
    "    y_n = rosenbrock(x_n)\n",
    "    # the item() attribute converts a single-element tensor into a float (i.e. a number that can be used)\n",
    "    plot(x_n.item(), y_n.item(), 'r*')\n",
    "\n",
    "    # This is our \"automatic differentiation\", which calculates the gradient\n",
    "    y_n.backward()\n",
    "    \n",
    "    # Now we don't want to compute gradients with respect to x_n, because\n",
    "    # in the equation after that would mean x_n has gradients with respect\n",
    "    # to previous versions of itself, which is not how gradient descent works.\n",
    "    #\n",
    "    # In other words, the new version of x_n is a simple scalar with no derivatives.\n",
    "    # Derivatives are then computed in the next iteration.\n",
    "    x_n.requires_grad=False\n",
    "    x_n = x_n - alpha * x_n.grad\n",
    "\n",
    "print(x_n.item())\n",
    "    \n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A more complex example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "#import torchvision\n",
    "#Image size\n",
    "size = 40\n",
    "\n",
    "#These are the x coordinates for every pixel\n",
    "numbers = list(range(0, size))\n",
    "square = [numbers for _ in numbers]\n",
    "x_coordinates = torch.tensor(square, dtype = torch.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following computes values of a Gaussian for the input. It assumes that `xs` corresponds to a square grid of pixels and contains only their $x$ coordinates, i.e. \n",
    "```\n",
    "0 1 2 3 4 \n",
    "0 1 2 3 4\n",
    "0 1 2 3 4\n",
    "0 1 2 3 4\n",
    "0 1 2 3 4\n",
    "```\n",
    "`pos` is the position as a Torch tensor and `sigma` is self explanatory.\n",
    "\n",
    "Torch is an array maths library, which means it supports operations such as scalar times array (both of course are called Tensors in Torch terminology, a scalar being a tensor containing one element), "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian(xs, pos, sigma):\n",
    "    #Generate Y coordinates\n",
    "    ys = xs.permute([1,0])\n",
    "    return torch.exp(-((pos[0] - xs)**2 + (pos[1]-ys)**2)/(2*sigma**2)) / (2*math.pi*sigma**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this to generate some simulated data, for example an image containing a Gaussian blob and some noise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9c5804c21fe7465bafa233b4ba22bb40"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "import math\n",
    "data = gaussian(x_coordinates, torch.tensor([25.5, 10.0]), 3.5) + torch.normal(torch.zeros([size,size]), 0.0005)\n",
    "figure()\n",
    "imshow(data)\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can find the position of this spot by fitting a Gaussian using least squares. This is an optimization procedure where we find the parameters of the Gaussian that minimize the sum of square errors. The sum of square errors being the thing we're minimizing is known as the *loss function*. Since it's an optimization problem we can solve it using gradient descent in Pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b5783f8b71f44254ac868590f26a7d5f"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Button(description='Click me a lot', style=ButtonStyle())",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3915d058d88d4560b2bbdb20fc97cd6a"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Output()",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0c8cb2a8a0c64067a2dc8a3e68ea75e9"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "#Now define a Gaussian spot to be optimised\n",
    "position = torch.tensor([10., 10.])\n",
    "sigma = torch.tensor([10.0])\n",
    "\n",
    "#Learning rates are always problem dependent. This rate of 1000 works \n",
    "#for this problem but is unusually large. Ususally learning rates are much less than 1.\n",
    "learning_rate=1000\n",
    "\n",
    "figure()\n",
    "def update_graph(_):\n",
    "    global position, sigma, learning_rate\n",
    "    #We want to optimize position and size\n",
    "    position.requires_grad = True\n",
    "    sigma.requires_grad = True\n",
    "    \n",
    "    #The model is also a single Gaussian\n",
    "    model = gaussian(x_coordinates, position, sigma)\n",
    "    #The sum square errors loss\n",
    "    loss = ((data-model)**2).sum()\n",
    "    \n",
    "    #Gradient descent\n",
    "    loss.backward()\n",
    "    position.requires_grad = False\n",
    "    sigma.requires_grad = False\n",
    "    \n",
    "    position = position - position.grad * learning_rate\n",
    "    sigma = sigma - sigma.grad * learning_rate\n",
    "\n",
    "    clf()\n",
    "    subplot(1,2, 1)\n",
    "    imshow(data.numpy())\n",
    "    title(\"data\")\n",
    "    subplot(1,2, 2)\n",
    "    imshow(model.detach().numpy())\n",
    "    title(\"model\")\n",
    "    suptitle(f\"position = {list(position.detach().numpy())}, sigma={sigma.item()}\")\n",
    "    \n",
    "update_graph(None)\n",
    "button = widgets.Button(description=\"Click me a lot\")\n",
    "output = widgets.Output()\n",
    "display(button, output)\n",
    "button.on_click(update_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch optimizers\n",
    "\n",
    "Writing gradient descent is quite repetitive, and it's the simplest of the gradient descent algorithms. Pytorch has number of optimizers built. Two of note are SGD and Adam. SGD is the classic stochastic gradient descent algorithm, which is gradient descent with momentum. Adam is another good choice, and no one will ever reject your paper for picking either of these two.\n",
    "\n",
    "Using optimizers is straightforward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b5b2ee4aa80b4fff95efc531eaccdeaa"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Button(description='Click me a lot', style=ButtonStyle())",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "83c2cc89c8a04ad288845c89edaa1d27"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Output()",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "26ce9918a6ca4dee84435f6dd924ff89"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "#Now define a Gaussian spot to be optimised\n",
    "position = torch.tensor([ 10., 10.], requires_grad=True)\n",
    "sigma = torch.tensor([10.0], requires_grad=True)\n",
    "learning_rate=1000\n",
    "\n",
    "figure()\n",
    "optimizer = torch.optim.SGD([position, sigma], lr = 1000)\n",
    "\n",
    "def update_graph(_):\n",
    "    global position, sigma\n",
    "\n",
    "    #pytorch will do very odd things if you forget this\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    model = gaussian(x_coordinates, position, sigma)\n",
    "    loss = ((data-model)**2).sum()\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    clf()\n",
    "    subplot(1,2, 1)\n",
    "    imshow(data.numpy())\n",
    "    title(\"data\")\n",
    "    subplot(1,2, 2)\n",
    "    imshow(model.detach().numpy())\n",
    "    title(\"model\")\n",
    "    suptitle(f\"position = {list(position.detach().numpy())}, sigma={sigma.item()}\")\n",
    "    \n",
    "update_graph(None)\n",
    "button = widgets.Button(description=\"Click me a lot\")\n",
    "output = widgets.Output()\n",
    "display(button, output)\n",
    "button.on_click(update_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit (windows store)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "metadata": {
   "interpreter": {
    "hash": "1f8d80d535cfd832283e4e3a1095d2ce45fe6627336684f2622a1965babb2f1c"
   }
  },
  "interpreter": {
   "hash": "1f8d80d535cfd832283e4e3a1095d2ce45fe6627336684f2622a1965babb2f1c"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}